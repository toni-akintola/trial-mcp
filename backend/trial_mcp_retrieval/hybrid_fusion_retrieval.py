__author__ = "qiao"

"""
Conduct the first stage retrieval by the hybrid retriever 
"""

from beir.datasets.data_loader import GenericDataLoader
import faiss
import json
from nltk import word_tokenize
import numpy as np
import os
from rank_bm25 import BM25Okapi
import sys
import tqdm
import torch
from transformers import AutoTokenizer, AutoModel
import argparse

import nltk

nltk.download("punkt", quiet=True)
nltk.download("averaged_perceptron_tagger", quiet=True)

# Determine device for PyTorch
if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")
elif torch.cuda.is_available():  # Fallback for other systems if CUDA were present
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
print(f"Using PyTorch device: {device}")


def get_bm25_corpus_index(corpus):
    corpus_path = os.path.join(os.path.dirname(__file__), f"bm25_corpus_{corpus}.json")

    # if already cached then load, otherwise build
    if os.path.exists(corpus_path):
        corpus_data = json.load(open(corpus_path))
        tokenized_corpus = corpus_data["tokenized_corpus"]
        corpus_nctids = corpus_data["corpus_nctids"]

    else:
        tokenized_corpus = []
        corpus_nctids = []

        # Use path relative to backend folder
        corpus_file_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            "dataset",
            corpus,
            "corpus.jsonl",
        )
        with open(corpus_file_path, "r") as f:
            for line in f.readlines():
                entry = json.loads(line)
                corpus_nctids.append(entry["_id"])

                # weighting: 3 * title, 2 * condition, 1 * text
                tokens = word_tokenize(entry["title"].lower()) * 3
                for disease in entry["metadata"]["diseases_list"]:
                    tokens += word_tokenize(disease.lower()) * 2
                tokens += word_tokenize(entry["text"].lower())

                tokenized_corpus.append(tokens)

        corpus_data = {
            "tokenized_corpus": tokenized_corpus,
            "corpus_nctids": corpus_nctids,
        }

        with open(corpus_path, "w") as f:
            json.dump(corpus_data, f, indent=4)

    bm25 = BM25Okapi(tokenized_corpus)

    return bm25, corpus_nctids


def get_medcpt_corpus_index(corpus):
    corpus_path = os.path.join(os.path.dirname(__file__), f"{corpus}_embeds.npy")
    nctids_path = os.path.join(os.path.dirname(__file__), f"{corpus}_nctids.json")

    # if already cached then load, otherwise build
    if os.path.exists(corpus_path):
        embeds = np.load(corpus_path)
        corpus_nctids = json.load(open(nctids_path))

    else:
        embeds = []
        corpus_nctids = []

        model = AutoModel.from_pretrained("ncbi/MedCPT-Article-Encoder").to(device)
        tokenizer = AutoTokenizer.from_pretrained("ncbi/MedCPT-Article-Encoder")

        # Use path relative to backend folder
        corpus_file_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            "dataset",
            corpus,
            "corpus.jsonl",
        )
        with open(corpus_file_path, "r") as f:
            print("Encoding the corpus")
            for line in tqdm.tqdm(f.readlines()):
                entry = json.loads(line)
                corpus_nctids.append(entry["_id"])

                title = entry["title"]
                text = entry["text"]

                with torch.no_grad():
                    # tokenize the articles
                    encoded = tokenizer(
                        [[title, text]],
                        truncation=True,
                        padding=True,
                        return_tensors="pt",
                        max_length=512,
                    ).to(device)

                    embed = model(**encoded).last_hidden_state[:, 0, :]

                    embeds.append(embed[0].cpu().numpy())

        embeds = np.array(embeds)

        np.save(corpus_path, embeds)
        with open(nctids_path, "w") as f:
            json.dump(corpus_nctids, f, indent=4)

    index = faiss.IndexFlatIP(768)
    index.add(embeds)

    return index, corpus_nctids


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Hybrid Fusion Retrieval for Clinical Trials."
    )
    parser.add_argument(
        "corpus", type=str, help="Corpus name (e.g., trec_2021, sigir)."
    )
    parser.add_argument(
        "keyword_file",
        type=str,
        help="Path to the keyword JSON file generated by keyword_generation.py.",
    )
    parser.add_argument("k", type=int, help="Parameter k for score calculation.")
    parser.add_argument("bm25_wt", type=int, help="Weight for BM25 retriever.")
    parser.add_argument("medcpt_wt", type=int, help="Weight for MedCPT retriever.")
    parser.add_argument(
        "--N_results", type=int, default=2000, help="Number of top results to retrieve."
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="default_model",
        help="Model name used for keyword generation (for output file naming consistency).",
    )

    args = parser.parse_args()

    corpus = args.corpus
    keyword_file_path = args.keyword_file
    k = args.k
    bm25_wt = args.bm25_wt
    medcpt_wt = args.medcpt_wt
    N = args.N_results
    model_name_for_output = args.model_name

    # Derive sample_suffix from the keyword_file_path for output naming
    base_keyword_filename = os.path.basename(keyword_file_path)
    # Example: retrieval_keywords_mcp_claude-3-sonnet-latest_sigir_sample2.json
    # Example: retrieval_keywords_mcp_claude-3-sonnet-latest_sigir.json

    sample_suffix = ""
    # Try to find "_sample<number>" at the end of the filename (before .json)
    parts = base_keyword_filename.replace(".json", "").split("_")
    if len(parts) > 0 and parts[-1].startswith("sample"):
        sample_suffix = "_" + parts[-1]

    # Output filename construction using args for clarity and consistency
    # The q_type (model) part of the name now comes from args.model_name
    output_path = os.path.join(
        os.path.dirname(os.path.dirname(__file__)),
        "results",
        f"retrieved_nctids_{model_name_for_output}_{corpus}{sample_suffix}.json",
    )
    print(f"Output will be saved to: {output_path}")

    # loading the qrels
    # Use path relative to backend folder
    data_folder = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), "dataset", corpus
    )
    _, _, qrels = GenericDataLoader(data_folder=data_folder).load(split="test")

    # loading all types of queries
    if not os.path.exists(keyword_file_path):
        print(f"Error: Keyword file not found at {keyword_file_path}")
        sys.exit(1)
    id2queries = json.load(open(keyword_file_path))

    # loading the indices
    bm25, bm25_nctids = get_bm25_corpus_index(corpus)
    medcpt, medcpt_nctids = get_medcpt_corpus_index(corpus)

    # loading the query encoder for MedCPT
    model = AutoModel.from_pretrained("ncbi/MedCPT-Query-Encoder").to(device)
    tokenizer = AutoTokenizer.from_pretrained("ncbi/MedCPT-Query-Encoder")

    # then conduct the searches, saving top N (e.g. 2000)
    qid2nctids = {}
    recalls = []

    # Use path relative to backend folder
    queries_file_path = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), "dataset", corpus, "queries.jsonl"
    )
    with open(queries_file_path, "r") as f:
        for line in tqdm.tqdm(f.readlines()):
            entry = json.loads(line)
            query = entry["text"]
            qid = entry["_id"]

            if qid not in qrels:
                continue

            truth_sum = sum(qrels[qid].values())

            # get the keyword list
            if qid not in id2queries or "conditions" not in id2queries[qid]:
                print(
                    f"Warning: No conditions found for qid {qid} in {keyword_file_path}. Skipping."
                )
                conditions = []
            else:
                conditions = id2queries[qid]["conditions"]

            if len(conditions) == 0:
                nctid2score = {}
            else:
                # a list of nctid lists for the bm25 retriever
                bm25_condition_top_nctids = []

                for condition in conditions:
                    tokens = word_tokenize(condition.lower())
                    top_nctids = bm25.get_top_n(tokens, bm25_nctids, n=N)
                    bm25_condition_top_nctids.append(top_nctids)

                # doing MedCPT retrieval
                with torch.no_grad():
                    encoded = tokenizer(
                        conditions,
                        truncation=True,
                        padding=True,
                        return_tensors="pt",
                        max_length=256,
                    ).to(device)

                    # encode the queries (use the [CLS] last hidden states as the representations)
                    embeds = model(**encoded).last_hidden_state[:, 0, :].cpu().numpy()

                    # search the Faiss index
                    scores, inds = medcpt.search(embeds, k=N)

                medcpt_condition_top_nctids = []
                for ind_list in inds:
                    top_nctids = [medcpt_nctids[ind] for ind in ind_list]
                    medcpt_condition_top_nctids.append(top_nctids)

                nctid2score = {}

                for condition_idx, (bm25_top_nctids, medcpt_top_nctids) in enumerate(
                    zip(bm25_condition_top_nctids, medcpt_condition_top_nctids)
                ):

                    if bm25_wt > 0:
                        for rank, nctid in enumerate(bm25_top_nctids):
                            if nctid not in nctid2score:
                                nctid2score[nctid] = 0

                            nctid2score[nctid] += (1 / (rank + k)) * (
                                1 / (condition_idx + 1)
                            )

                    if medcpt_wt > 0:
                        for rank, nctid in enumerate(medcpt_top_nctids):
                            if nctid not in nctid2score:
                                nctid2score[nctid] = 0

                            nctid2score[nctid] += (1 / (rank + k)) * (
                                1 / (condition_idx + 1)
                            )

            nctid2score = sorted(nctid2score.items(), key=lambda x: -x[1])
            top_nctids = [nctid for nctid, _ in nctid2score[:N]]
            qid2nctids[qid] = top_nctids

            actual_sum = sum([qrels[qid].get(nctid, 0) for nctid in top_nctids])
            recalls.append(actual_sum / truth_sum)

    with open(output_path, "w") as f:
        json.dump(qid2nctids, f, indent=4)
